{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tyler Faulkner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import warnings\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000\n",
    "\n",
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/data/cs3450/data/cifar100_flattened_testing.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "       https://d2l.ai/chapter_deep-learning-computation/use-gpu.html\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "DEVICE=try_gpu()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is the square example that we looked at in class.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only to a numpy array\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.detach().numpy(), y=y.detach().numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for Fashion-MNIST training\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select your datasource.\n",
    "dataset = 'Fashion-MNIST'\n",
    "# dataset = 'CIFAR-10'\n",
    "# dataset = 'CIFAR-100'\n",
    "\n",
    "x_train_lin, y_train_lin = create_linear_training_data()\n",
    "#x_train, y_train = create_folded_training_data()\n",
    "#points_train, insideness_train = create_square()\n",
    "x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "\n",
    "# Move selected datasets to GPU\n",
    "x_train = x_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)\n",
    "\n",
    "\n",
    "# Move selected datasets to GPU\n",
    "x_train_lin = x_train_lin.to(DEVICE)\n",
    "y_train_lin = y_train_lin.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for Fashion-MNIST testing\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the accuracy of your network\n",
    "x_test_lin, y_test_lin = create_linear_training_data()\n",
    "x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)\n",
    "\n",
    "# Move the selected datasets to the GPU\n",
    "x_test = x_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)\n",
    "\n",
    "x_test_lin = x_test_lin.to(DEVICE)\n",
    "y_test_lin = y_test_lin.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer(W, x, b):\n",
    "    return W@x + b\n",
    "\n",
    "def RELU(layer):\n",
    "    return layer*(layer>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59.6000],\n",
       "        [48.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test 1\n",
    "x=torch.tensor([[3.],[4.]])\n",
    "W1 = torch.tensor([[1.,2.],[0.,3.],[-2.,1.]])\n",
    "W2 = torch.tensor([[2.,3.,-2.],[4.,0.,1.]])\n",
    "b1 = torch.tensor([[0.5],[1.2],[3.]])\n",
    "b2 = torch.tensor([[-1.],[1.]])\n",
    "\n",
    "L = compute_layer(W1, x, b1)\n",
    "\n",
    "compute_layer(W2, L, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60.9000],\n",
       "        [27.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test 2\n",
    "x=torch.tensor([[1.],[3.]])\n",
    "W1 = torch.tensor([[-2.,4.],[3.,2.],[1.,1.]])\n",
    "W2 = torch.tensor([[3.,3.,4.],[2.,-1.,3.]])\n",
    "b1 = torch.tensor([[0.7],[-4.],[0.2]])\n",
    "b2 = torch.tensor([[-3.],[-2.]])\n",
    "\n",
    "L = compute_layer(W1, x, b1)\n",
    "\n",
    "compute_layer(W2, L, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((3,2), requires_grad=True, device=DEVICE)\n",
    "W2 = torch.randn((2,3), requires_grad=True, device=DEVICE)\n",
    "b1 = torch.zeros((3,1), requires_grad=True, device=DEVICE)\n",
    "b2 = torch.zeros((2,1), requires_grad=True, device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    W1 *= 0.1\n",
    "    W2 *= 0.1\n",
    "\n",
    "l_rate = 0.0001\n",
    "alpha = 0.00001\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Training\n",
    "\n",
    "def learn_linear(x_train, y_train):\n",
    "    W1 = torch.randn((3,2), requires_grad=True, device=DEVICE)\n",
    "    W2 = torch.randn((2,3), requires_grad=True, device=DEVICE)\n",
    "    b1 = torch.zeros((3,1), requires_grad=True, device=DEVICE)\n",
    "    b2 = torch.zeros((2,1), requires_grad=True, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W1 *= 0.1\n",
    "        W2 *= 0.1\n",
    "\n",
    "    l_rate = 0.0001\n",
    "    alpha = 0.00001\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for s in range(batch_size, x_train.size()[1], batch_size):\n",
    "            sample = x_train[:, s-batch_size:s]\n",
    "            y_sample = y_train[:, s-batch_size:s]\n",
    "\n",
    "            L= RELU(compute_layer(W1, sample, b1))\n",
    "\n",
    "            O = compute_layer(W2, L, b2)\n",
    "\n",
    "            Error = 1/len(y_sample)*torch.sum((y_sample-O)**2)\n",
    "\n",
    "            Regularization = W1.data.pow(2).sum() + W2.data.pow(2).sum()\n",
    "\n",
    "            Loss = Error + alpha*Regularization\n",
    "\n",
    "            Loss.backward()\n",
    "\n",
    "            #print(Loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                W1 -= alpha*W1.grad\n",
    "                W2 -= alpha*W2.grad\n",
    "                b1 -= alpha*b1.grad\n",
    "                b2 -= alpha*b2.grad\n",
    "\n",
    "            W1.grad.zero_()\n",
    "            W2.grad.zero_()\n",
    "            b2.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "    L= RELU(compute_layer(W1, x_train, b1))\n",
    "    O = compute_layer(W2, L, b2)\n",
    "\n",
    "    predicted_classes = torch.argmax(O,0)\n",
    "    true_classes = torch.argmax(y_train,0)\n",
    "    total = y_train.size()[1]\n",
    "    print(\"Accuracy: \", ((predicted_classes==true_classes).sum()/total).item())\n",
    "    print()\n",
    "    print(\"Matrix used to generate data:\")\n",
    "    print(W2 @ W1)\n",
    "    print(Loss)\n",
    "#learn_linear(x_train_lin, y_train_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689, 0.9526, 0.5000],\n",
       "        [0.7311, 0.0474, 0.5000]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def SOFTMAX(out):\n",
    "    z = out - torch.max(out, 0).values\n",
    "    num = torch.exp(z)\n",
    "    denom = num.sum(axis=0)\n",
    "    return num/denom\n",
    "print(DEVICE)\n",
    "SOFTMAX(torch.tensor([[1.,6.,5.],[2.,3.,5.]], device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden(Layer):\n",
    "    def __init__(self, neurons, last_layer_neurons, activation=RELU, DEVICE=DEVICE):\n",
    "        super().__init__(neurons)\n",
    "        self.input = last_layer_neurons\n",
    "        self.W = torch.randn((neurons, last_layer_neurons), requires_grad=True, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            self.W *= 0.1\n",
    "        self.b = torch.zeros((neurons, 1), requires_grad=True, device=DEVICE)\n",
    "        self.activation = activation\n",
    "    def get_W(self):\n",
    "        return self.W\n",
    "    def forward(self, x):\n",
    "        if self.activation != None:\n",
    "            return self.activation(self.W.double()@x + self.b.double())\n",
    "        else:\n",
    "            return self.W@x + self.b\n",
    "    def backward(self, lr):\n",
    "        with torch.no_grad():\n",
    "            self.W -= lr * self.W.grad\n",
    "            self.b -= lr * self.b.grad\n",
    "        self.W.grad.zero_()\n",
    "        self.b.grad.zero_()\n",
    "    def print(self):\n",
    "        print(\"Hidden layer with\", self.neurons, \" output neurons,\", self.input, \" input neurons,\", self.activation.__name__, \"activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(Hidden):\n",
    "    def __init__(self, neurons, last_layer_neurons, activation=SOFTMAX, DEVICE=DEVICE):\n",
    "        super().__init__(neurons, last_layer_neurons, activation, DEVICE)\n",
    "    def print(self):\n",
    "        act_name = \"None\"\n",
    "        if self.activation != None:\n",
    "            act_name=self.activation.__name__\n",
    "        print(\"Output layer with\", self.neurons, \" output neurons,\", self.input, \" input neurons,\", act_name, \"activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, layers, reg_coef, lr, loss=\"l2\"):\n",
    "        self.layers = layers\n",
    "        self.output = None\n",
    "        self.reg_coef = reg_coef\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "    def _loss(self, final, s_out):\n",
    "        reg = self._reg()\n",
    "        if self.loss==\"l2\":\n",
    "            error = self._l2(final, s_out)\n",
    "        elif self.loss==\"cross\":\n",
    "            error = self._cross(final, s_out)\n",
    "        loss = error + self.reg_coef*reg\n",
    "        return loss\n",
    "    \n",
    "    def _cross(self, output, true):\n",
    "        #1e-7 if going to zero\n",
    "        z = torch.sum((true*torch.log(output)))\n",
    "        return -z/true.size()[1]\n",
    "    \n",
    "    def _l2(self, sample, s_out):\n",
    "        return 1/len(s_out)*torch.sum((s_out-sample)**2)\n",
    "    \n",
    "    def _reg(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.layers)):\n",
    "            total += self.layers[i].get_W().data.pow(2).sum()\n",
    "        return total\n",
    "    \n",
    "    def _backprop(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].backward(self.lr)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(len(self.layers)):\n",
    "            out = self.layers[i].forward(out)\n",
    "        return out\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        with torch.no_grad():\n",
    "                output = (self.forward(x))\n",
    "                loss = self._loss(output, y).item()\n",
    "                print(\"Loss \", loss)\n",
    "                predicted_classes = torch.argmax(output,0)\n",
    "                true_classes = torch.argmax(y,0)\n",
    "                total = y.size()[1]\n",
    "                accuracy = ((predicted_classes==true_classes).sum()/total).item()\n",
    "                print(\"Accuracy: \", accuracy)\n",
    "                print()\n",
    "                return accuracy, loss\n",
    "    def train(self, epochs, batch_size, x_train, y_train):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            entries = x_train.size()[1]\n",
    "            front_p = 0\n",
    "            end_p = batch_size\n",
    "            if(batch_size > x_train.size()[1]):\n",
    "                end_p = x_train.size()[1]\n",
    "            while front_p < x_train.size()[1]:\n",
    "                sample = x_train[:, front_p:end_p]\n",
    "                y_sample = y_train[:, front_p:end_p]\n",
    "                final = self.forward(sample)\n",
    "                self._loss(final, y_sample).backward()\n",
    "                self._backprop()\n",
    "                end_p += batch_size\n",
    "                front_p += batch_size\n",
    "                if end_p > x_train.size()[1]:\n",
    "                    end_p = x_train.size()[1]\n",
    "            print(\"Epoch\", e+1, \"results:\")\n",
    "            acc, loss = self.test(x_train, y_train)\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "        return accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_neurons, input_nodes, output_nodes, reg_coef, lr, loss, last_activation):\n",
    "    layers = [Hidden(hidden_neurons[0], input_nodes)]\n",
    "    for i in range(1, len(hidden_neurons)):\n",
    "        n = hidden_neurons[i]\n",
    "        layer = Hidden(n, layers[i-1].neurons)\n",
    "        layers.append(layer)\n",
    "    out = Output(output_nodes, layers[len(layers)-1].neurons)\n",
    "    layers.append(Output(output_nodes, layers[len(layers)-1].neurons, activation=last_activation))\n",
    "    for i in layers:\n",
    "        i.print()\n",
    "    return Network(layers, reg_coef, lr, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_test(hidden_neurons, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test):\n",
    "    network = create_model(hidden_neurons, x_train.size()[0], y_train.size()[0], reg_coef, lr, loss, last_activation)\n",
    "    accuracies = None\n",
    "    start = time.perf_counter()\n",
    "    accuracies, losses = network.train(epochs, batch_size, x_train.double(), y_train)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    print(\"Test Set Accuracy\")\n",
    "    network.test(x_test.double(), y_test)\n",
    "    print(\"The training took\", end-start, \"seconds to finish\")\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    epoch_axis = range(1, len(accuracies)+1)\n",
    "    ax1.plot(epoch_axis, accuracies, label=\"accuracy\")\n",
    "    ax1.scatter(epoch_axis, accuracies, s=7, color='g')\n",
    "\n",
    "    ax2 = ax1.twinx() \n",
    "    ax2.plot(epoch_axis, losses, c='r', label=\"loss\")\n",
    "    ax2.scatter(epoch_axis, losses, s=7, c='y')\n",
    "    plt.title(\"Accuracy Plot\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#First Attempt\n",
    "reg_coef = 0.0\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 2\n",
    "reg_coef = 0.0\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 90\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Attempt 3\n",
    "reg_coef = 0.0001\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 90\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 4\n",
    "reg_coef = 0.0001\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 180\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 5\n",
    "reg_coef = 0.0001\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 240\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 6\n",
    "reg_coef = 0.0001\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "epochs = 240\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 7\n",
    "reg_coef = 0.0001\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 8\n",
    "reg_coef = 0.0001\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "layers = [28, 14, 10]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt 9\n",
    "reg_coef = 0.0001\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "layers = [14, 7]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer with 14  output neurons, 784  input neurons, RELU activation function\n",
      "Hidden layer with 7  output neurons, 14  input neurons, RELU activation function\n",
      "Output layer with 10  output neurons, 7  input neurons, SOFTMAX activation function\n",
      "Epoch 1 results:\n",
      "Loss  2.3119078294009245\n",
      "Accuracy:  0.14733333333333334\n",
      "\n",
      "Epoch 2 results:\n",
      "Loss  2.3085772770600936\n",
      "Accuracy:  0.17101666666666668\n",
      "\n",
      "Epoch 3 results:\n",
      "Loss  2.304304189158646\n",
      "Accuracy:  0.1764\n",
      "\n",
      "Epoch 4 results:\n",
      "Loss  2.2981232906027085\n",
      "Accuracy:  0.17556666666666668\n",
      "\n",
      "Epoch 5 results:\n",
      "Loss  2.28850781566751\n",
      "Accuracy:  0.17195000000000002\n",
      "\n",
      "Epoch 6 results:\n",
      "Loss  2.2734574103331737\n",
      "Accuracy:  0.17308333333333334\n",
      "\n",
      "Epoch 7 results:\n",
      "Loss  2.250436178039167\n",
      "Accuracy:  0.17995\n",
      "\n",
      "Epoch 8 results:\n",
      "Loss  2.2160219966026182\n",
      "Accuracy:  0.19838333333333336\n",
      "\n",
      "Epoch 9 results:\n",
      "Loss  2.16672721991862\n",
      "Accuracy:  0.22428333333333333\n",
      "\n",
      "Epoch 10 results:\n",
      "Loss  2.099891666436261\n",
      "Accuracy:  0.25253333333333333\n",
      "\n",
      "Epoch 11 results:\n",
      "Loss  2.011761031228887\n",
      "Accuracy:  0.27371666666666666\n",
      "\n",
      "Epoch 12 results:\n",
      "Loss  1.8966280974562642\n",
      "Accuracy:  0.28453333333333336\n",
      "\n",
      "Epoch 13 results:\n",
      "Loss  1.7581101582908314\n",
      "Accuracy:  0.3105833333333333\n",
      "\n",
      "Epoch 14 results:\n",
      "Loss  1.616337553949231\n",
      "Accuracy:  0.3561666666666667\n",
      "\n",
      "Epoch 15 results:\n",
      "Loss  1.4905078298372438\n",
      "Accuracy:  0.4651166666666667\n",
      "\n",
      "Epoch 16 results:\n",
      "Loss  1.3878703959084153\n",
      "Accuracy:  0.49443333333333334\n",
      "\n",
      "Epoch 17 results:\n",
      "Loss  1.3082762355302229\n",
      "Accuracy:  0.49623333333333336\n",
      "\n",
      "Epoch 18 results:\n",
      "Loss  1.2480999987398735\n",
      "Accuracy:  0.5012\n",
      "\n",
      "Epoch 19 results:\n",
      "Loss  1.202235763096211\n",
      "Accuracy:  0.5092166666666667\n",
      "\n",
      "Epoch 20 results:\n",
      "Loss  1.1659917048421662\n",
      "Accuracy:  0.5165666666666667\n",
      "\n",
      "Epoch 21 results:\n",
      "Loss  1.1362236149762452\n",
      "Accuracy:  0.52475\n",
      "\n",
      "Epoch 22 results:\n",
      "Loss  1.110969844085933\n",
      "Accuracy:  0.5329666666666667\n",
      "\n",
      "Epoch 23 results:\n",
      "Loss  1.0888663624166293\n",
      "Accuracy:  0.5426500000000001\n",
      "\n",
      "Epoch 24 results:\n",
      "Loss  1.0688176290454698\n",
      "Accuracy:  0.5572166666666667\n",
      "\n",
      "Epoch 25 results:\n",
      "Loss  1.0502216862081855\n",
      "Accuracy:  0.57815\n",
      "\n",
      "Epoch 26 results:\n",
      "Loss  1.0329506297851836\n",
      "Accuracy:  0.5964833333333334\n",
      "\n",
      "Epoch 27 results:\n",
      "Loss  1.016763156421097\n",
      "Accuracy:  0.6114833333333334\n",
      "\n",
      "Epoch 28 results:\n",
      "Loss  1.001446047052288\n",
      "Accuracy:  0.6196166666666667\n",
      "\n",
      "Epoch 29 results:\n",
      "Loss  0.9869164983715967\n",
      "Accuracy:  0.62485\n",
      "\n",
      "Epoch 30 results:\n",
      "Loss  0.9731944566644023\n",
      "Accuracy:  0.6275000000000001\n",
      "\n",
      "Epoch 31 results:\n",
      "Loss  0.9603865347175159\n",
      "Accuracy:  0.6300333333333333\n",
      "\n",
      "Epoch 32 results:\n",
      "Loss  0.9485943204983366\n",
      "Accuracy:  0.6324333333333334\n",
      "\n",
      "Epoch 33 results:\n",
      "Loss  0.9378290942667057\n",
      "Accuracy:  0.6344833333333334\n",
      "\n",
      "Epoch 34 results:\n",
      "Loss  0.9280778563573191\n",
      "Accuracy:  0.6366166666666667\n",
      "\n",
      "Epoch 35 results:\n",
      "Loss  0.9192620608829964\n",
      "Accuracy:  0.6385166666666667\n",
      "\n",
      "Epoch 36 results:\n",
      "Loss  0.9112649182900354\n",
      "Accuracy:  0.6410166666666667\n",
      "\n",
      "Epoch 37 results:\n",
      "Loss  0.9039754921394242\n",
      "Accuracy:  0.6427666666666667\n",
      "\n",
      "Epoch 38 results:\n",
      "Loss  0.897279072472401\n",
      "Accuracy:  0.645\n",
      "\n",
      "Epoch 39 results:\n",
      "Loss  0.8910709136960069\n",
      "Accuracy:  0.6467\n",
      "\n",
      "Epoch 40 results:\n",
      "Loss  0.8852729113692713\n",
      "Accuracy:  0.6488166666666667\n",
      "\n",
      "Epoch 41 results:\n",
      "Loss  0.8798088137397072\n",
      "Accuracy:  0.6519\n",
      "\n",
      "Epoch 42 results:\n",
      "Loss  0.874627227286865\n",
      "Accuracy:  0.6542666666666667\n",
      "\n",
      "Epoch 43 results:\n",
      "Loss  0.869685718786161\n",
      "Accuracy:  0.65705\n",
      "\n",
      "Epoch 44 results:\n",
      "Loss  0.8649523071396372\n",
      "Accuracy:  0.6597000000000001\n",
      "\n",
      "Epoch 45 results:\n",
      "Loss  0.8603925938286773\n",
      "Accuracy:  0.6624166666666667\n",
      "\n",
      "Epoch 46 results:\n",
      "Loss  0.8559928777660952\n",
      "Accuracy:  0.6656000000000001\n",
      "\n",
      "Epoch 47 results:\n",
      "Loss  0.8517291649054055\n",
      "Accuracy:  0.6686833333333334\n",
      "\n",
      "Epoch 48 results:\n",
      "Loss  0.8475913616710932\n",
      "Accuracy:  0.6712833333333333\n",
      "\n",
      "Epoch 49 results:\n",
      "Loss  0.8435603742893795\n",
      "Accuracy:  0.6736333333333334\n",
      "\n",
      "Epoch 50 results:\n",
      "Loss  0.8396311575643818\n",
      "Accuracy:  0.6769000000000001\n",
      "\n",
      "Epoch 51 results:\n",
      "Loss  0.8357984967148603\n",
      "Accuracy:  0.67935\n",
      "\n",
      "Epoch 52 results:\n",
      "Loss  0.8320590716076616\n",
      "Accuracy:  0.6823333333333333\n",
      "\n",
      "Epoch 53 results:\n",
      "Loss  0.8284083947433918\n",
      "Accuracy:  0.6849166666666667\n",
      "\n",
      "Epoch 54 results:\n",
      "Loss  0.8248429976233858\n",
      "Accuracy:  0.6874333333333333\n",
      "\n",
      "Epoch 55 results:\n",
      "Loss  0.8213607368623821\n",
      "Accuracy:  0.6896333333333333\n",
      "\n",
      "Epoch 56 results:\n",
      "Loss  0.8179569827244755\n",
      "Accuracy:  0.6917500000000001\n",
      "\n",
      "Epoch 57 results:\n",
      "Loss  0.8146323992758804\n",
      "Accuracy:  0.6936\n",
      "\n",
      "Epoch 58 results:\n",
      "Loss  0.8113826068846421\n",
      "Accuracy:  0.6957166666666666\n",
      "\n",
      "Epoch 59 results:\n",
      "Loss  0.8082096285283549\n",
      "Accuracy:  0.6977833333333334\n",
      "\n",
      "Epoch 60 results:\n",
      "Loss  0.8051133730194737\n",
      "Accuracy:  0.6995166666666667\n",
      "\n",
      "Epoch 61 results:\n",
      "Loss  0.8020896555221567\n",
      "Accuracy:  0.7013166666666667\n",
      "\n",
      "Epoch 62 results:\n",
      "Loss  0.7991390055057821\n",
      "Accuracy:  0.7028833333333334\n",
      "\n",
      "Epoch 63 results:\n",
      "Loss  0.7962614042564273\n",
      "Accuracy:  0.7048\n",
      "\n",
      "Epoch 64 results:\n",
      "Loss  0.7934560366070939\n",
      "Accuracy:  0.7064666666666667\n",
      "\n",
      "Epoch 65 results:\n",
      "Loss  0.7907154601460227\n",
      "Accuracy:  0.70835\n",
      "\n",
      "Epoch 66 results:\n",
      "Loss  0.7880373118373686\n",
      "Accuracy:  0.70975\n",
      "\n",
      "Epoch 67 results:\n",
      "Loss  0.7854192649801104\n",
      "Accuracy:  0.7114833333333334\n",
      "\n",
      "Epoch 68 results:\n",
      "Loss  0.7828605121501911\n",
      "Accuracy:  0.7129666666666667\n",
      "\n",
      "Epoch 69 results:\n",
      "Loss  0.780361241526503\n",
      "Accuracy:  0.7146\n",
      "\n",
      "Epoch 70 results:\n",
      "Loss  0.7779146924893736\n",
      "Accuracy:  0.7159500000000001\n",
      "\n",
      "Epoch 71 results:\n",
      "Loss  0.7755189000980394\n",
      "Accuracy:  0.7174166666666667\n",
      "\n",
      "Epoch 72 results:\n",
      "Loss  0.7731830793678572\n",
      "Accuracy:  0.7186\n",
      "\n",
      "Epoch 73 results:\n",
      "Loss  0.7708949428823038\n",
      "Accuracy:  0.7200666666666667\n",
      "\n",
      "Epoch 74 results:\n",
      "Loss  0.7686603567431884\n",
      "Accuracy:  0.7212666666666667\n",
      "\n",
      "Epoch 75 results:\n",
      "Loss  0.7664672929840609\n",
      "Accuracy:  0.7227333333333333\n",
      "\n",
      "Epoch 76 results:\n",
      "Loss  0.7643285851658943\n",
      "Accuracy:  0.7236666666666667\n",
      "\n",
      "Epoch 77 results:\n",
      "Loss  0.7622447951778827\n",
      "Accuracy:  0.7250666666666667\n",
      "\n",
      "Epoch 78 results:\n",
      "Loss  0.7602260319091367\n",
      "Accuracy:  0.7261833333333334\n",
      "\n",
      "Epoch 79 results:\n",
      "Loss  0.7582545604522143\n",
      "Accuracy:  0.727\n",
      "\n",
      "Epoch 80 results:\n",
      "Loss  0.7563307493562742\n",
      "Accuracy:  0.7280333333333334\n",
      "\n",
      "Epoch 81 results:\n",
      "Loss  0.7544460723099359\n",
      "Accuracy:  0.7293000000000001\n",
      "\n",
      "Epoch 82 results:\n",
      "Loss  0.7526091198363509\n",
      "Accuracy:  0.7302833333333334\n",
      "\n",
      "Epoch 83 results:\n",
      "Loss  0.7508128550476151\n",
      "Accuracy:  0.73085\n",
      "\n",
      "Epoch 84 results:\n",
      "Loss  0.749058724465932\n",
      "Accuracy:  0.7319\n",
      "\n",
      "Epoch 85 results:\n",
      "Loss  0.747341865466387\n",
      "Accuracy:  0.73285\n",
      "\n",
      "Epoch 86 results:\n",
      "Loss  0.7456621517049058\n",
      "Accuracy:  0.7335833333333334\n",
      "\n",
      "Epoch 87 results:\n",
      "Loss  0.7440194711391763\n",
      "Accuracy:  0.7343333333333334\n",
      "\n",
      "Epoch 88 results:\n",
      "Loss  0.7424054152744811\n",
      "Accuracy:  0.7350166666666667\n",
      "\n",
      "Epoch 89 results:\n",
      "Loss  0.7408229885806943\n",
      "Accuracy:  0.7357333333333334\n",
      "\n",
      "Epoch 90 results:\n",
      "Loss  0.7392734561202032\n",
      "Accuracy:  0.7365\n",
      "\n",
      "Epoch 91 results:\n",
      "Loss  0.7377501909665916\n",
      "Accuracy:  0.7371666666666667\n",
      "\n",
      "Epoch 92 results:\n",
      "Loss  0.7362557202000066\n",
      "Accuracy:  0.7377\n",
      "\n",
      "Epoch 93 results:\n",
      "Loss  0.7347896071307812\n",
      "Accuracy:  0.73855\n",
      "\n",
      "Epoch 94 results:\n",
      "Loss  0.7333474024043821\n",
      "Accuracy:  0.7396\n",
      "\n",
      "Epoch 95 results:\n",
      "Loss  0.7319309187016468\n",
      "Accuracy:  0.7403333333333334\n",
      "\n",
      "Epoch 96 results:\n",
      "Loss  0.7305386647558533\n",
      "Accuracy:  0.7412666666666667\n",
      "\n",
      "Epoch 97 results:\n",
      "Loss  0.7291679868225898\n",
      "Accuracy:  0.74205\n",
      "\n",
      "Epoch 98 results:\n",
      "Loss  0.7278187190800455\n",
      "Accuracy:  0.7426\n",
      "\n",
      "Epoch 99 results:\n",
      "Loss  0.7264906330267091\n",
      "Accuracy:  0.7433500000000001\n",
      "\n",
      "Epoch 100 results:\n",
      "Loss  0.7251833749445764\n",
      "Accuracy:  0.7437166666666667\n",
      "\n",
      "Epoch 101 results:\n",
      "Loss  0.7238960481101716\n",
      "Accuracy:  0.7443000000000001\n",
      "\n",
      "Epoch 102 results:\n",
      "Loss  0.7226301429365672\n",
      "Accuracy:  0.7448333333333333\n",
      "\n",
      "Epoch 103 results:\n",
      "Loss  0.7213837143975135\n",
      "Accuracy:  0.7455166666666667\n",
      "\n",
      "Epoch 104 results:\n",
      "Loss  0.7201556669804163\n",
      "Accuracy:  0.74595\n",
      "\n",
      "Epoch 105 results:\n",
      "Loss  0.7189451780056922\n",
      "Accuracy:  0.7464166666666667\n",
      "\n",
      "Epoch 106 results:\n",
      "Loss  0.7177515637275763\n",
      "Accuracy:  0.7469333333333333\n",
      "\n",
      "Epoch 107 results:\n",
      "Loss  0.7165713844545484\n",
      "Accuracy:  0.7472166666666668\n",
      "\n",
      "Epoch 108 results:\n",
      "Loss  0.715410444918017\n",
      "Accuracy:  0.7480333333333333\n",
      "\n",
      "Epoch 109 results:\n",
      "Loss  0.7142652077389117\n",
      "Accuracy:  0.7485666666666667\n",
      "\n",
      "Epoch 110 results:\n",
      "Loss  0.7131376809067376\n",
      "Accuracy:  0.7492333333333334\n",
      "\n",
      "Epoch 111 results:\n",
      "Loss  0.7120252804963385\n",
      "Accuracy:  0.7498\n",
      "\n",
      "Epoch 112 results:\n",
      "Loss  0.7109275320208132\n",
      "Accuracy:  0.7500333333333333\n",
      "\n",
      "Epoch 113 results:\n",
      "Loss  0.7098438630653672\n",
      "Accuracy:  0.7504666666666667\n",
      "\n",
      "Epoch 114 results:\n",
      "Loss  0.708773595133483\n",
      "Accuracy:  0.7509166666666667\n",
      "\n",
      "Epoch 115 results:\n",
      "Loss  0.7077212905912089\n",
      "Accuracy:  0.7515166666666667\n",
      "\n",
      "Epoch 116 results:\n",
      "Loss  0.7066756265212312\n",
      "Accuracy:  0.7519333333333333\n",
      "\n",
      "Epoch 117 results:\n",
      "Loss  0.7056397558043093\n",
      "Accuracy:  0.7522500000000001\n",
      "\n",
      "Epoch 118 results:\n",
      "Loss  0.7046197490645979\n",
      "Accuracy:  0.7526\n",
      "\n",
      "Epoch 119 results:\n",
      "Loss  0.7036085191484082\n",
      "Accuracy:  0.7530833333333333\n",
      "\n",
      "Epoch 120 results:\n",
      "Loss  0.7026106768349875\n",
      "Accuracy:  0.7535166666666667\n",
      "\n",
      "Epoch 121 results:\n",
      "Loss  0.7016251299229876\n",
      "Accuracy:  0.7537833333333334\n",
      "\n",
      "Epoch 122 results:\n",
      "Loss  0.7006532191043303\n",
      "Accuracy:  0.7542666666666668\n",
      "\n",
      "Epoch 123 results:\n",
      "Loss  0.699690427732064\n",
      "Accuracy:  0.7546666666666667\n",
      "\n",
      "Epoch 124 results:\n",
      "Loss  0.6987416230951361\n",
      "Accuracy:  0.755\n",
      "\n",
      "Epoch 125 results:\n",
      "Loss  0.6978040723002272\n",
      "Accuracy:  0.7555000000000001\n",
      "\n",
      "Epoch 126 results:\n",
      "Loss  0.696872167504241\n",
      "Accuracy:  0.7558166666666667\n",
      "\n",
      "Epoch 127 results:\n",
      "Loss  0.6959514905805387\n",
      "Accuracy:  0.7560833333333333\n",
      "\n",
      "Epoch 128 results:\n",
      "Loss  0.6950404661486236\n",
      "Accuracy:  0.7562333333333334\n",
      "\n",
      "Epoch 129 results:\n",
      "Loss  0.6941379649288827\n",
      "Accuracy:  0.75675\n",
      "\n",
      "Epoch 130 results:\n",
      "Loss  0.6932415628470202\n",
      "Accuracy:  0.7570666666666667\n",
      "\n",
      "Epoch 131 results:\n",
      "Loss  0.6923538191115378\n",
      "Accuracy:  0.7571833333333333\n",
      "\n",
      "Epoch 132 results:\n",
      "Loss  0.6914743251334239\n",
      "Accuracy:  0.7573666666666667\n",
      "\n",
      "Epoch 133 results:\n",
      "Loss  0.6906073824567192\n",
      "Accuracy:  0.7576166666666667\n",
      "\n",
      "Epoch 134 results:\n",
      "Loss  0.6897450769372611\n",
      "Accuracy:  0.75795\n",
      "\n",
      "Epoch 135 results:\n",
      "Loss  0.6888898863246593\n",
      "Accuracy:  0.7586\n",
      "\n",
      "Epoch 136 results:\n",
      "Loss  0.6880424356341777\n",
      "Accuracy:  0.7588666666666667\n",
      "\n",
      "Epoch 137 results:\n",
      "Loss  0.6872003230406462\n",
      "Accuracy:  0.7593666666666667\n",
      "\n",
      "Epoch 138 results:\n",
      "Loss  0.6863652425236438\n",
      "Accuracy:  0.75975\n",
      "\n",
      "Epoch 139 results:\n",
      "Loss  0.6855366124803939\n",
      "Accuracy:  0.7600666666666667\n",
      "\n",
      "Epoch 140 results:\n",
      "Loss  0.6847137104118882\n",
      "Accuracy:  0.7605500000000001\n",
      "\n",
      "Epoch 141 results:\n",
      "Loss  0.6838957206995427\n",
      "Accuracy:  0.7607833333333334\n",
      "\n",
      "Epoch 142 results:\n",
      "Loss  0.6830844868672254\n",
      "Accuracy:  0.7611166666666667\n",
      "\n",
      "Epoch 143 results:\n",
      "Loss  0.6822781672443494\n",
      "Accuracy:  0.7615333333333334\n",
      "\n",
      "Epoch 144 results:\n",
      "Loss  0.6814787095831611\n",
      "Accuracy:  0.7618833333333334\n",
      "\n",
      "Epoch 145 results:\n",
      "Loss  0.6806845936404133\n",
      "Accuracy:  0.7622\n",
      "\n",
      "Epoch 146 results:\n",
      "Loss  0.6798986937250598\n",
      "Accuracy:  0.7624500000000001\n",
      "\n",
      "Epoch 147 results:\n",
      "Loss  0.6791167202832701\n",
      "Accuracy:  0.7629166666666667\n",
      "\n",
      "Epoch 148 results:\n",
      "Loss  0.678340497801208\n",
      "Accuracy:  0.7631833333333333\n",
      "\n",
      "Epoch 149 results:\n",
      "Loss  0.6775681188665746\n",
      "Accuracy:  0.7635166666666667\n",
      "\n",
      "Epoch 150 results:\n",
      "Loss  0.6768029604277354\n",
      "Accuracy:  0.7637333333333334\n",
      "\n",
      "Epoch 151 results:\n",
      "Loss  0.6760449556830777\n",
      "Accuracy:  0.7639333333333334\n",
      "\n",
      "Epoch 152 results:\n",
      "Loss  0.6752884931436934\n",
      "Accuracy:  0.7640833333333333\n",
      "\n",
      "Epoch 153 results:\n",
      "Loss  0.6745352161599287\n",
      "Accuracy:  0.7645000000000001\n",
      "\n",
      "Epoch 154 results:\n",
      "Loss  0.6737879827196688\n",
      "Accuracy:  0.7648\n",
      "\n",
      "Epoch 155 results:\n",
      "Loss  0.6730443388350527\n",
      "Accuracy:  0.76525\n",
      "\n",
      "Epoch 156 results:\n",
      "Loss  0.6723054190927322\n",
      "Accuracy:  0.7656166666666667\n",
      "\n",
      "Epoch 157 results:\n",
      "Loss  0.67157061548182\n",
      "Accuracy:  0.7659166666666667\n",
      "\n",
      "Epoch 158 results:\n",
      "Loss  0.670844891375861\n",
      "Accuracy:  0.7662\n",
      "\n",
      "Epoch 159 results:\n",
      "Loss  0.6700876637197273\n",
      "Accuracy:  0.7664000000000001\n",
      "\n",
      "Epoch 160 results:\n",
      "Loss  0.6693637398134201\n",
      "Accuracy:  0.7667166666666667\n",
      "\n",
      "Epoch 161 results:\n",
      "Loss  0.6686454161430678\n",
      "Accuracy:  0.7668666666666667\n",
      "\n",
      "Epoch 162 results:\n",
      "Loss  0.6679302247280764\n",
      "Accuracy:  0.7671166666666667\n",
      "\n",
      "Epoch 163 results:\n",
      "Loss  0.6672295598269776\n",
      "Accuracy:  0.7673666666666668\n",
      "\n",
      "Epoch 164 results:\n",
      "Loss  0.6665242655016865\n",
      "Accuracy:  0.7677\n",
      "\n",
      "Epoch 165 results:\n",
      "Loss  0.6658202517162451\n",
      "Accuracy:  0.7678666666666667\n",
      "\n",
      "Epoch 166 results:\n",
      "Loss  0.6651193486286047\n",
      "Accuracy:  0.7681833333333333\n",
      "\n",
      "Epoch 167 results:\n",
      "Loss  0.6644218627993679\n",
      "Accuracy:  0.7684333333333334\n",
      "\n",
      "Epoch 168 results:\n",
      "Loss  0.6637268370165061\n",
      "Accuracy:  0.76875\n",
      "\n",
      "Epoch 169 results:\n",
      "Loss  0.663032575547852\n",
      "Accuracy:  0.7690833333333333\n",
      "\n",
      "Epoch 170 results:\n",
      "Loss  0.662344863737668\n",
      "Accuracy:  0.7695166666666667\n",
      "\n",
      "Epoch 171 results:\n",
      "Loss  0.6616595571152115\n",
      "Accuracy:  0.7696833333333334\n",
      "\n",
      "Epoch 172 results:\n",
      "Loss  0.6609760959076548\n",
      "Accuracy:  0.7697666666666667\n",
      "\n",
      "Epoch 173 results:\n",
      "Loss  0.660295420368343\n",
      "Accuracy:  0.7699666666666667\n",
      "\n",
      "Epoch 174 results:\n",
      "Loss  0.6596159204302192\n",
      "Accuracy:  0.7703333333333334\n",
      "\n",
      "Epoch 175 results:\n",
      "Loss  0.6589400294701488\n",
      "Accuracy:  0.7705666666666667\n",
      "\n",
      "Epoch 176 results:\n",
      "Loss  0.6582646402817541\n",
      "Accuracy:  0.7708833333333334\n",
      "\n",
      "Epoch 177 results:\n",
      "Loss  0.6575912194682558\n",
      "Accuracy:  0.7712833333333333\n",
      "\n",
      "Epoch 178 results:\n",
      "Loss  0.6569194437262138\n",
      "Accuracy:  0.7714166666666668\n",
      "\n",
      "Epoch 179 results:\n",
      "Loss  0.6562490323076383\n",
      "Accuracy:  0.77175\n",
      "\n",
      "Epoch 180 results:\n",
      "Loss  0.6555779202464879\n",
      "Accuracy:  0.7719333333333334\n",
      "\n",
      "Epoch 181 results:\n",
      "Loss  0.6549057582244754\n",
      "Accuracy:  0.7720166666666667\n",
      "\n",
      "Epoch 182 results:\n",
      "Loss  0.65423191471335\n",
      "Accuracy:  0.77225\n",
      "\n",
      "Epoch 183 results:\n",
      "Loss  0.653558259516496\n",
      "Accuracy:  0.7725666666666667\n",
      "\n",
      "Epoch 184 results:\n",
      "Loss  0.6528832134704768\n",
      "Accuracy:  0.7728166666666667\n",
      "\n",
      "Epoch 185 results:\n",
      "Loss  0.6522053991778373\n",
      "Accuracy:  0.7730333333333334\n",
      "\n",
      "Epoch 186 results:\n",
      "Loss  0.651523940812622\n",
      "Accuracy:  0.7732666666666667\n",
      "\n",
      "Epoch 187 results:\n",
      "Loss  0.650837462386768\n",
      "Accuracy:  0.7735166666666667\n",
      "\n",
      "Epoch 188 results:\n",
      "Loss  0.6501427114653796\n",
      "Accuracy:  0.7737166666666667\n",
      "\n",
      "Epoch 189 results:\n",
      "Loss  0.6494410273571742\n",
      "Accuracy:  0.7742166666666667\n",
      "\n",
      "Epoch 190 results:\n",
      "Loss  0.6487324299166222\n",
      "Accuracy:  0.7745333333333334\n",
      "\n",
      "Epoch 191 results:\n",
      "Loss  0.6480074157558923\n",
      "Accuracy:  0.7748\n",
      "\n",
      "Epoch 192 results:\n",
      "Loss  0.6472649433037267\n",
      "Accuracy:  0.7751166666666667\n",
      "\n",
      "Epoch 193 results:\n",
      "Loss  0.646512341575566\n",
      "Accuracy:  0.7754666666666667\n",
      "\n",
      "Epoch 194 results:\n",
      "Loss  0.6457401254122814\n",
      "Accuracy:  0.7757000000000001\n",
      "\n",
      "Epoch 195 results:\n",
      "Loss  0.6449388889660668\n",
      "Accuracy:  0.77635\n",
      "\n",
      "Epoch 196 results:\n",
      "Loss  0.6440997991811238\n",
      "Accuracy:  0.7768333333333334\n",
      "\n",
      "Epoch 197 results:\n",
      "Loss  0.643265285330834\n",
      "Accuracy:  0.7773166666666667\n",
      "\n",
      "Epoch 198 results:\n",
      "Loss  0.6424410167696086\n",
      "Accuracy:  0.7775333333333334\n",
      "\n",
      "Epoch 199 results:\n",
      "Loss  0.6416191634596689\n",
      "Accuracy:  0.7779166666666667\n",
      "\n",
      "Epoch 200 results:\n",
      "Loss  0.6407934640352669\n",
      "Accuracy:  0.77805\n",
      "\n",
      "Epoch 201 results:\n",
      "Loss  0.639963777766038\n",
      "Accuracy:  0.77825\n",
      "\n",
      "Epoch 202 results:\n",
      "Loss  0.6391224576000059\n",
      "Accuracy:  0.7786166666666667\n",
      "\n",
      "Epoch 203 results:\n",
      "Loss  0.6382764771127385\n",
      "Accuracy:  0.77905\n",
      "\n",
      "Epoch 204 results:\n",
      "Loss  0.637417728317966\n",
      "Accuracy:  0.7794333333333334\n",
      "\n",
      "Epoch 205 results:\n",
      "Loss  0.6365495797840974\n",
      "Accuracy:  0.7799166666666667\n",
      "\n",
      "Epoch 206 results:\n",
      "Loss  0.6356731476809511\n",
      "Accuracy:  0.7801333333333333\n",
      "\n",
      "Epoch 207 results:\n",
      "Loss  0.6347814594064694\n",
      "Accuracy:  0.7805166666666667\n",
      "\n",
      "Epoch 208 results:\n",
      "Loss  0.6338753109097408\n",
      "Accuracy:  0.7809\n",
      "\n",
      "Epoch 209 results:\n",
      "Loss  0.6329634498321705\n",
      "Accuracy:  0.7812166666666667\n",
      "\n",
      "Epoch 210 results:\n",
      "Loss  0.6320367250357686\n",
      "Accuracy:  0.7815833333333334\n",
      "\n",
      "Epoch 211 results:\n",
      "Loss  0.6311015445558723\n",
      "Accuracy:  0.7819\n",
      "\n",
      "Epoch 212 results:\n",
      "Loss  0.630150701002151\n",
      "Accuracy:  0.7822333333333333\n",
      "\n",
      "Epoch 213 results:\n",
      "Loss  0.629185958512882\n",
      "Accuracy:  0.7826666666666667\n",
      "\n",
      "Epoch 214 results:\n",
      "Loss  0.6282136240405011\n",
      "Accuracy:  0.7831333333333333\n",
      "\n",
      "Epoch 215 results:\n",
      "Loss  0.6272314102385789\n",
      "Accuracy:  0.7833166666666667\n",
      "\n",
      "Epoch 216 results:\n",
      "Loss  0.6262378041363513\n",
      "Accuracy:  0.7836000000000001\n",
      "\n",
      "Epoch 217 results:\n",
      "Loss  0.625235540298115\n",
      "Accuracy:  0.7841166666666667\n",
      "\n",
      "Epoch 218 results:\n",
      "Loss  0.6242167779189942\n",
      "Accuracy:  0.7845000000000001\n",
      "\n",
      "Epoch 219 results:\n",
      "Loss  0.6231882171722074\n",
      "Accuracy:  0.7849166666666667\n",
      "\n",
      "Epoch 220 results:\n",
      "Loss  0.6221485752430783\n",
      "Accuracy:  0.7854500000000001\n",
      "\n",
      "Epoch 221 results:\n",
      "Loss  0.6210988361821842\n",
      "Accuracy:  0.78585\n",
      "\n",
      "Epoch 222 results:\n",
      "Loss  0.6200447421975663\n",
      "Accuracy:  0.7864166666666667\n",
      "\n",
      "Epoch 223 results:\n",
      "Loss  0.6189820769929368\n",
      "Accuracy:  0.7868833333333334\n",
      "\n",
      "Epoch 224 results:\n",
      "Loss  0.6179100555373416\n",
      "Accuracy:  0.7875333333333334\n",
      "\n",
      "Epoch 225 results:\n",
      "Loss  0.6168410755416327\n",
      "Accuracy:  0.7878333333333334\n",
      "\n",
      "Epoch 226 results:\n",
      "Loss  0.6157617950956601\n",
      "Accuracy:  0.7884\n",
      "\n",
      "Epoch 227 results:\n",
      "Loss  0.6146777236435368\n",
      "Accuracy:  0.7887166666666667\n",
      "\n",
      "Epoch 228 results:\n",
      "Loss  0.6136051556577741\n",
      "Accuracy:  0.7892166666666667\n",
      "\n",
      "Epoch 229 results:\n",
      "Loss  0.612535845648934\n",
      "Accuracy:  0.7896333333333334\n",
      "\n",
      "Epoch 230 results:\n",
      "Loss  0.6114706820357619\n",
      "Accuracy:  0.7900833333333334\n",
      "\n",
      "Epoch 231 results:\n",
      "Loss  0.6104006075957552\n",
      "Accuracy:  0.79045\n",
      "\n",
      "Epoch 232 results:\n",
      "Loss  0.6093356244081528\n",
      "Accuracy:  0.7906833333333334\n",
      "\n",
      "Epoch 233 results:\n",
      "Loss  0.6082700120191264\n",
      "Accuracy:  0.7912666666666667\n",
      "\n",
      "Epoch 234 results:\n",
      "Loss  0.6072081255091829\n",
      "Accuracy:  0.7916333333333334\n",
      "\n",
      "Epoch 235 results:\n",
      "Loss  0.6061518110699143\n",
      "Accuracy:  0.7920166666666667\n",
      "\n",
      "Epoch 236 results:\n",
      "Loss  0.6051086025137162\n",
      "Accuracy:  0.7925666666666668\n",
      "\n",
      "Epoch 237 results:\n",
      "Loss  0.6040596795692698\n",
      "Accuracy:  0.79295\n",
      "\n",
      "Epoch 238 results:\n",
      "Loss  0.603028192695001\n",
      "Accuracy:  0.79325\n",
      "\n",
      "Epoch 239 results:\n",
      "Loss  0.6020045616816372\n",
      "Accuracy:  0.7936333333333334\n",
      "\n",
      "Epoch 240 results:\n",
      "Loss  0.6009919579611904\n",
      "Accuracy:  0.79395\n",
      "\n",
      "Epoch 241 results:\n",
      "Loss  0.5999873291639084\n",
      "Accuracy:  0.79415\n",
      "\n",
      "Epoch 242 results:\n",
      "Loss  0.5989886518298064\n",
      "Accuracy:  0.7944166666666667\n",
      "\n",
      "Epoch 243 results:\n",
      "Loss  0.5979845205711628\n",
      "Accuracy:  0.7947833333333334\n",
      "\n",
      "Epoch 244 results:\n",
      "Loss  0.5970097407997341\n",
      "Accuracy:  0.7953333333333333\n",
      "\n",
      "Epoch 245 results:\n",
      "Loss  0.5960452440245442\n",
      "Accuracy:  0.7957333333333334\n",
      "\n",
      "Epoch 246 results:\n",
      "Loss  0.5950960916803537\n",
      "Accuracy:  0.7963833333333333\n",
      "\n",
      "Epoch 247 results:\n",
      "Loss  0.5941545739308792\n",
      "Accuracy:  0.7967833333333334\n",
      "\n",
      "Epoch 248 results:\n",
      "Loss  0.5932376315783247\n",
      "Accuracy:  0.7973\n",
      "\n",
      "Epoch 249 results:\n",
      "Loss  0.592321196527769\n",
      "Accuracy:  0.7977333333333334\n",
      "\n",
      "Epoch 250 results:\n",
      "Loss  0.5914095323296588\n",
      "Accuracy:  0.7980666666666667\n",
      "\n",
      "Epoch 251 results:\n",
      "Loss  0.5905260927116328\n",
      "Accuracy:  0.7982833333333333\n",
      "\n",
      "Epoch 252 results:\n",
      "Loss  0.5896468510292997\n",
      "Accuracy:  0.7986166666666668\n",
      "\n",
      "Epoch 253 results:\n",
      "Loss  0.5887786358664535\n",
      "Accuracy:  0.7988500000000001\n",
      "\n",
      "Epoch 254 results:\n",
      "Loss  0.5879187611924498\n",
      "Accuracy:  0.79925\n",
      "\n",
      "Epoch 255 results:\n",
      "Loss  0.5870670601508188\n",
      "Accuracy:  0.7996500000000001\n",
      "\n",
      "Epoch 256 results:\n",
      "Loss  0.5862288022278388\n",
      "Accuracy:  0.8000833333333334\n",
      "\n",
      "Epoch 257 results:\n",
      "Loss  0.5853968733450969\n",
      "Accuracy:  0.8005166666666667\n",
      "\n",
      "Epoch 258 results:\n",
      "Loss  0.5845813369352633\n",
      "Accuracy:  0.8009666666666667\n",
      "\n",
      "Epoch 259 results:\n",
      "Loss  0.5837606371352453\n",
      "Accuracy:  0.8014666666666667\n",
      "\n",
      "Epoch 260 results:\n",
      "Loss  0.58294782467606\n",
      "Accuracy:  0.8017666666666667\n",
      "\n",
      "Epoch 261 results:\n",
      "Loss  0.5821467254476861\n",
      "Accuracy:  0.8022833333333333\n",
      "\n",
      "Epoch 262 results:\n",
      "Loss  0.5813580640511135\n",
      "Accuracy:  0.8024666666666667\n",
      "\n",
      "Epoch 263 results:\n",
      "Loss  0.5805780058201219\n",
      "Accuracy:  0.8027000000000001\n",
      "\n",
      "Epoch 264 results:\n",
      "Loss  0.5798231502482933\n",
      "Accuracy:  0.8028833333333334\n",
      "\n",
      "Epoch 265 results:\n",
      "Loss  0.5790514948734945\n",
      "Accuracy:  0.8033\n",
      "\n",
      "Epoch 266 results:\n",
      "Loss  0.5782895352469535\n",
      "Accuracy:  0.8037833333333334\n",
      "\n",
      "Epoch 267 results:\n",
      "Loss  0.5775309817150098\n",
      "Accuracy:  0.80405\n",
      "\n",
      "Epoch 268 results:\n",
      "Loss  0.5767957826784793\n",
      "Accuracy:  0.80415\n",
      "\n",
      "Epoch 269 results:\n",
      "Loss  0.5760559757880013\n",
      "Accuracy:  0.8046500000000001\n",
      "\n",
      "Epoch 270 results:\n",
      "Loss  0.575322790939092\n",
      "Accuracy:  0.8048666666666667\n",
      "\n",
      "Epoch 271 results:\n",
      "Loss  0.5746049092000971\n",
      "Accuracy:  0.8052\n",
      "\n",
      "Epoch 272 results:\n",
      "Loss  0.5738830302640211\n",
      "Accuracy:  0.8055166666666667\n",
      "\n",
      "Epoch 273 results:\n",
      "Loss  0.5731693489265663\n",
      "Accuracy:  0.8058333333333334\n",
      "\n",
      "Epoch 274 results:\n",
      "Loss  0.5724589817428838\n",
      "Accuracy:  0.80615\n",
      "\n",
      "Epoch 275 results:\n",
      "Loss  0.5717548704689545\n",
      "Accuracy:  0.8065333333333333\n",
      "\n",
      "Epoch 276 results:\n",
      "Loss  0.5710584356990751\n",
      "Accuracy:  0.8067500000000001\n",
      "\n",
      "Epoch 277 results:\n",
      "Loss  0.5703715870231595\n",
      "Accuracy:  0.8069833333333334\n",
      "\n",
      "Epoch 278 results:\n",
      "Loss  0.569697756730875\n",
      "Accuracy:  0.8073333333333333\n",
      "\n",
      "Epoch 279 results:\n",
      "Loss  0.5690163507783849\n",
      "Accuracy:  0.8077500000000001\n",
      "\n",
      "Epoch 280 results:\n",
      "Loss  0.5683450621343629\n",
      "Accuracy:  0.8080166666666667\n",
      "\n",
      "Epoch 281 results:\n",
      "Loss  0.5676782774540876\n",
      "Accuracy:  0.80825\n",
      "\n",
      "Epoch 282 results:\n",
      "Loss  0.567017461953302\n",
      "Accuracy:  0.8086333333333334\n",
      "\n",
      "Epoch 283 results:\n",
      "Loss  0.5663603314002242\n",
      "Accuracy:  0.8089833333333334\n",
      "\n",
      "Epoch 284 results:\n",
      "Loss  0.5657110280983293\n",
      "Accuracy:  0.8091166666666667\n",
      "\n",
      "Epoch 285 results:\n",
      "Loss  0.5650686489155601\n",
      "Accuracy:  0.8094666666666667\n",
      "\n",
      "Epoch 286 results:\n",
      "Loss  0.5644261761812953\n",
      "Accuracy:  0.8096500000000001\n",
      "\n",
      "Epoch 287 results:\n",
      "Loss  0.563788873885897\n",
      "Accuracy:  0.8100666666666667\n",
      "\n",
      "Epoch 288 results:\n",
      "Loss  0.563158050016792\n",
      "Accuracy:  0.8102166666666667\n",
      "\n",
      "Epoch 289 results:\n",
      "Loss  0.5625578322805965\n",
      "Accuracy:  0.8102833333333334\n",
      "\n",
      "Epoch 290 results:\n",
      "Loss  0.5619349057669082\n",
      "Accuracy:  0.8104\n",
      "\n",
      "Epoch 291 results:\n",
      "Loss  0.5613161881665684\n",
      "Accuracy:  0.8107333333333334\n",
      "\n",
      "Epoch 292 results:\n",
      "Loss  0.5607024894016454\n",
      "Accuracy:  0.8110666666666667\n",
      "\n",
      "Epoch 293 results:\n",
      "Loss  0.5600746741644508\n",
      "Accuracy:  0.8113333333333334\n",
      "\n",
      "Epoch 294 results:\n",
      "Loss  0.5594596838120034\n",
      "Accuracy:  0.8116166666666667\n",
      "\n",
      "Epoch 295 results:\n",
      "Loss  0.558859237585824\n",
      "Accuracy:  0.8120333333333334\n",
      "\n",
      "Epoch 296 results:\n",
      "Loss  0.5582596494264802\n",
      "Accuracy:  0.8122833333333334\n",
      "\n",
      "Epoch 297 results:\n",
      "Loss  0.5576847886177229\n",
      "Accuracy:  0.8124833333333333\n",
      "\n",
      "Epoch 298 results:\n",
      "Loss  0.5570976378297555\n",
      "Accuracy:  0.8128500000000001\n",
      "\n",
      "Epoch 299 results:\n",
      "Loss  0.5565147239101441\n",
      "Accuracy:  0.8130333333333334\n",
      "\n",
      "Epoch 300 results:\n",
      "Loss  0.5559365891247208\n",
      "Accuracy:  0.8132833333333334\n",
      "\n",
      "Epoch 301 results:\n",
      "Loss  0.555360652889056\n",
      "Accuracy:  0.8135166666666667\n",
      "\n",
      "Epoch 302 results:\n",
      "Loss  0.5547938138831724\n",
      "Accuracy:  0.8136166666666667\n",
      "\n",
      "Epoch 303 results:\n",
      "Loss  0.5542238537304269\n",
      "Accuracy:  0.8136\n",
      "\n",
      "Epoch 304 results:\n",
      "Loss  0.5536603910721021\n",
      "Accuracy:  0.8139000000000001\n",
      "\n",
      "Epoch 305 results:\n",
      "Loss  0.5531056302176872\n",
      "Accuracy:  0.81405\n",
      "\n",
      "Epoch 306 results:\n",
      "Loss  0.5525636256339186\n",
      "Accuracy:  0.8142\n",
      "\n",
      "Epoch 307 results:\n",
      "Loss  0.5520126305943694\n",
      "Accuracy:  0.8144166666666667\n",
      "\n",
      "Epoch 308 results:\n",
      "Loss  0.5514690285093835\n",
      "Accuracy:  0.8146166666666667\n",
      "\n",
      "Epoch 309 results:\n",
      "Loss  0.5509357023784975\n",
      "Accuracy:  0.8148500000000001\n",
      "\n",
      "Epoch 310 results:\n",
      "Loss  0.5503963038250207\n",
      "Accuracy:  0.8151\n",
      "\n",
      "Epoch 311 results:\n",
      "Loss  0.5498403846737989\n",
      "Accuracy:  0.8152\n",
      "\n",
      "Epoch 312 results:\n",
      "Loss  0.549311532111855\n",
      "Accuracy:  0.81565\n",
      "\n",
      "Epoch 313 results:\n",
      "Loss  0.5487849724067067\n",
      "Accuracy:  0.8159666666666667\n",
      "\n",
      "Epoch 314 results:\n",
      "Loss  0.5482578768474163\n",
      "Accuracy:  0.8162833333333334\n",
      "\n",
      "Epoch 315 results:\n",
      "Loss  0.5477390305060391\n",
      "Accuracy:  0.8164333333333333\n",
      "\n",
      "Epoch 316 results:\n",
      "Loss  0.547231402620441\n",
      "Accuracy:  0.8166833333333334\n",
      "\n",
      "Epoch 317 results:\n",
      "Loss  0.5467086788061789\n",
      "Accuracy:  0.8166833333333334\n",
      "\n",
      "Epoch 318 results:\n",
      "Loss  0.5461990385236721\n",
      "Accuracy:  0.8167666666666668\n",
      "\n",
      "Epoch 319 results:\n",
      "Loss  0.5456949878526152\n",
      "Accuracy:  0.8168833333333334\n",
      "\n",
      "Epoch 320 results:\n",
      "Loss  0.5451931334833926\n",
      "Accuracy:  0.8171166666666667\n",
      "\n",
      "Epoch 321 results:\n",
      "Loss  0.5446960582552006\n",
      "Accuracy:  0.8172833333333334\n",
      "\n",
      "Epoch 322 results:\n",
      "Loss  0.5441979740863889\n",
      "Accuracy:  0.8173833333333334\n",
      "\n",
      "Epoch 323 results:\n",
      "Loss  0.5437067287648003\n",
      "Accuracy:  0.8173166666666667\n",
      "\n",
      "Epoch 324 results:\n",
      "Loss  0.5432169293346969\n",
      "Accuracy:  0.8176\n",
      "\n",
      "Epoch 325 results:\n",
      "Loss  0.5427325080420408\n",
      "Accuracy:  0.8177000000000001\n",
      "\n",
      "Epoch 326 results:\n",
      "Loss  0.54225136683388\n",
      "Accuracy:  0.8178666666666667\n",
      "\n",
      "Epoch 327 results:\n",
      "Loss  0.5417748231272821\n",
      "Accuracy:  0.8181166666666667\n",
      "\n",
      "Epoch 328 results:\n",
      "Loss  0.5412998954358732\n",
      "Accuracy:  0.81835\n",
      "\n",
      "Epoch 329 results:\n",
      "Loss  0.5408271459662974\n",
      "Accuracy:  0.8184166666666667\n",
      "\n",
      "Epoch 330 results:\n",
      "Loss  0.5403572614618625\n",
      "Accuracy:  0.8187333333333334\n",
      "\n",
      "Epoch 331 results:\n",
      "Loss  0.5398926032930144\n",
      "Accuracy:  0.8189666666666667\n",
      "\n",
      "Epoch 332 results:\n",
      "Loss  0.5394318563807275\n",
      "Accuracy:  0.8190833333333334\n",
      "\n",
      "Epoch 333 results:\n",
      "Loss  0.5389746034168776\n",
      "Accuracy:  0.8192666666666667\n",
      "\n",
      "Epoch 334 results:\n",
      "Loss  0.5385199175605336\n",
      "Accuracy:  0.8195166666666667\n",
      "\n",
      "Epoch 335 results:\n",
      "Loss  0.5380633945176841\n",
      "Accuracy:  0.8197666666666668\n",
      "\n",
      "Epoch 336 results:\n",
      "Loss  0.5376111908455173\n",
      "Accuracy:  0.8199833333333334\n",
      "\n",
      "Epoch 337 results:\n",
      "Loss  0.53716713232487\n",
      "Accuracy:  0.8201666666666667\n",
      "\n",
      "Epoch 338 results:\n",
      "Loss  0.5367352159713862\n",
      "Accuracy:  0.8201666666666667\n",
      "\n",
      "Epoch 339 results:\n",
      "Loss  0.536296205326355\n",
      "Accuracy:  0.8202666666666667\n",
      "\n",
      "Epoch 340 results:\n",
      "Loss  0.5358739376403345\n",
      "Accuracy:  0.8204166666666667\n",
      "\n",
      "Epoch 341 results:\n",
      "Loss  0.5354386284177334\n",
      "Accuracy:  0.8205666666666667\n",
      "\n",
      "Epoch 342 results:\n",
      "Loss  0.5350076760177315\n",
      "Accuracy:  0.8206833333333333\n",
      "\n",
      "Epoch 343 results:\n",
      "Loss  0.5345787427336349\n",
      "Accuracy:  0.8208500000000001\n",
      "\n",
      "Epoch 344 results:\n",
      "Loss  0.534152086358906\n",
      "Accuracy:  0.8209333333333334\n",
      "\n",
      "Epoch 345 results:\n",
      "Loss  0.5337297448986\n",
      "Accuracy:  0.8209833333333334\n",
      "\n",
      "Epoch 346 results:\n",
      "Loss  0.5333109939261077\n",
      "Accuracy:  0.8211\n",
      "\n",
      "Epoch 347 results:\n",
      "Loss  0.5328941767526728\n",
      "Accuracy:  0.82135\n",
      "\n",
      "Epoch 348 results:\n",
      "Loss  0.532478091375468\n",
      "Accuracy:  0.8215166666666667\n",
      "\n",
      "Epoch 349 results:\n",
      "Loss  0.5320674836690211\n",
      "Accuracy:  0.8217\n",
      "\n",
      "Epoch 350 results:\n",
      "Loss  0.5316627663546927\n",
      "Accuracy:  0.8217833333333334\n",
      "\n",
      "Epoch 351 results:\n",
      "Loss  0.5312723538843502\n",
      "Accuracy:  0.8220333333333334\n",
      "\n",
      "Epoch 352 results:\n",
      "Loss  0.5308704763301116\n",
      "Accuracy:  0.8221333333333334\n",
      "\n",
      "Epoch 353 results:\n",
      "Loss  0.5304720742845939\n",
      "Accuracy:  0.8222666666666667\n",
      "\n",
      "Epoch 354 results:\n",
      "Loss  0.5300635842987408\n",
      "Accuracy:  0.8224666666666667\n",
      "\n",
      "Epoch 355 results:\n",
      "Loss  0.5296671157571605\n",
      "Accuracy:  0.8225666666666667\n",
      "\n",
      "Epoch 356 results:\n",
      "Loss  0.5292734754096786\n",
      "Accuracy:  0.8227333333333334\n",
      "\n",
      "Epoch 357 results:\n",
      "Loss  0.5288836247457194\n",
      "Accuracy:  0.8228666666666667\n",
      "\n",
      "Epoch 358 results:\n",
      "Loss  0.5284924811269749\n",
      "Accuracy:  0.8229833333333334\n",
      "\n",
      "Epoch 359 results:\n",
      "Loss  0.5281036676745747\n",
      "Accuracy:  0.8231333333333334\n",
      "\n",
      "Epoch 360 results:\n",
      "Loss  0.5277122935095182\n",
      "Accuracy:  0.8232666666666667\n",
      "\n",
      "Epoch 361 results:\n",
      "Loss  0.5273263879700605\n",
      "Accuracy:  0.82345\n",
      "\n",
      "Epoch 362 results:\n",
      "Loss  0.5269415445655133\n",
      "Accuracy:  0.8236\n",
      "\n",
      "Epoch 363 results:\n",
      "Loss  0.5265614497050127\n",
      "Accuracy:  0.8237666666666668\n",
      "\n",
      "Epoch 364 results:\n",
      "Loss  0.5261881469268787\n",
      "Accuracy:  0.8240000000000001\n",
      "\n",
      "Epoch 365 results:\n",
      "Loss  0.5258146644568484\n",
      "Accuracy:  0.8241833333333334\n",
      "\n",
      "Epoch 366 results:\n",
      "Loss  0.5254422514971598\n",
      "Accuracy:  0.8241333333333334\n",
      "\n",
      "Epoch 367 results:\n",
      "Loss  0.5250780655515563\n",
      "Accuracy:  0.82445\n",
      "\n",
      "Epoch 368 results:\n",
      "Loss  0.5247100486556548\n",
      "Accuracy:  0.8246833333333333\n",
      "\n",
      "Epoch 369 results:\n",
      "Loss  0.5243447498181845\n",
      "Accuracy:  0.8248500000000001\n",
      "\n",
      "Epoch 370 results:\n",
      "Loss  0.5239813643947036\n",
      "Accuracy:  0.8249333333333334\n",
      "\n",
      "Epoch 371 results:\n",
      "Loss  0.5236189662044116\n",
      "Accuracy:  0.8251666666666667\n",
      "\n",
      "Epoch 372 results:\n",
      "Loss  0.5232534226438755\n",
      "Accuracy:  0.8253\n",
      "\n",
      "Epoch 373 results:\n",
      "Loss  0.5228979226921067\n",
      "Accuracy:  0.8254\n",
      "\n",
      "Epoch 374 results:\n",
      "Loss  0.5225421571562325\n",
      "Accuracy:  0.8255833333333333\n",
      "\n",
      "Epoch 375 results:\n",
      "Loss  0.5221887656195879\n",
      "Accuracy:  0.8257166666666667\n",
      "\n",
      "Epoch 376 results:\n",
      "Loss  0.5218389009953972\n",
      "Accuracy:  0.8258833333333334\n",
      "\n",
      "Epoch 377 results:\n",
      "Loss  0.5214910796198281\n",
      "Accuracy:  0.8260166666666667\n",
      "\n",
      "Epoch 378 results:\n",
      "Loss  0.5211434637699246\n",
      "Accuracy:  0.8262666666666667\n",
      "\n",
      "Epoch 379 results:\n",
      "Loss  0.5207992028902677\n",
      "Accuracy:  0.8265166666666667\n",
      "\n",
      "Epoch 380 results:\n",
      "Loss  0.5204563787030373\n",
      "Accuracy:  0.8266333333333333\n",
      "\n",
      "Epoch 381 results:\n",
      "Loss  0.52011770710373\n",
      "Accuracy:  0.8268166666666668\n",
      "\n",
      "Epoch 382 results:\n",
      "Loss  0.5197809018071631\n",
      "Accuracy:  0.8269500000000001\n",
      "\n",
      "Epoch 383 results:\n",
      "Loss  0.5194437925320466\n",
      "Accuracy:  0.8271166666666667\n",
      "\n",
      "Epoch 384 results:\n",
      "Loss  0.5191108813157909\n",
      "Accuracy:  0.82735\n",
      "\n",
      "Epoch 385 results:\n",
      "Loss  0.5187801126388484\n",
      "Accuracy:  0.8274666666666667\n",
      "\n",
      "Epoch 386 results:\n",
      "Loss  0.518431373924655\n",
      "Accuracy:  0.8276833333333333\n",
      "\n",
      "Epoch 387 results:\n",
      "Loss  0.5181057465865155\n",
      "Accuracy:  0.8278000000000001\n",
      "\n",
      "Epoch 388 results:\n",
      "Loss  0.5177784927835296\n",
      "Accuracy:  0.8279500000000001\n",
      "\n",
      "Epoch 389 results:\n",
      "Loss  0.5174485168712154\n",
      "Accuracy:  0.8281000000000001\n",
      "\n",
      "Epoch 390 results:\n",
      "Loss  0.5171216700589312\n",
      "Accuracy:  0.8282333333333334\n",
      "\n",
      "Epoch 391 results:\n",
      "Loss  0.5167940646572365\n",
      "Accuracy:  0.8283833333333334\n",
      "\n",
      "Epoch 392 results:\n",
      "Loss  0.5164721032639276\n",
      "Accuracy:  0.8285333333333333\n",
      "\n",
      "Epoch 393 results:\n",
      "Loss  0.5161518946209186\n",
      "Accuracy:  0.8285666666666667\n",
      "\n",
      "Epoch 394 results:\n",
      "Loss  0.5158297672688305\n",
      "Accuracy:  0.82865\n",
      "\n",
      "Epoch 395 results:\n",
      "Loss  0.5155088193756205\n",
      "Accuracy:  0.82875\n",
      "\n",
      "Epoch 396 results:\n",
      "Loss  0.5151873641187238\n",
      "Accuracy:  0.8288833333333334\n",
      "\n",
      "Epoch 397 results:\n",
      "Loss  0.5148692340885694\n",
      "Accuracy:  0.8290833333333334\n",
      "\n",
      "Epoch 398 results:\n",
      "Loss  0.5145503733350671\n",
      "Accuracy:  0.8292\n",
      "\n",
      "Epoch 399 results:\n",
      "Loss  0.5142358269878115\n",
      "Accuracy:  0.8293333333333334\n",
      "\n",
      "Epoch 400 results:\n",
      "Loss  0.5139216257963968\n",
      "Accuracy:  0.8295166666666667\n",
      "\n",
      "Epoch 401 results:\n",
      "Loss  0.5135986898453021\n",
      "Accuracy:  0.8296166666666667\n",
      "\n",
      "Epoch 402 results:\n",
      "Loss  0.5132896306623631\n",
      "Accuracy:  0.8297\n",
      "\n",
      "Epoch 403 results:\n",
      "Loss  0.5129780363697242\n",
      "Accuracy:  0.8298166666666668\n",
      "\n",
      "Epoch 404 results:\n",
      "Loss  0.5126685466964765\n",
      "Accuracy:  0.8299000000000001\n",
      "\n",
      "Epoch 405 results:\n",
      "Loss  0.5123610222391612\n",
      "Accuracy:  0.8300500000000001\n",
      "\n",
      "Epoch 406 results:\n",
      "Loss  0.5120538954157747\n",
      "Accuracy:  0.8300666666666667\n",
      "\n",
      "Epoch 407 results:\n",
      "Loss  0.5117497651582265\n",
      "Accuracy:  0.8302\n",
      "\n",
      "Epoch 408 results:\n",
      "Loss  0.5114446321850684\n",
      "Accuracy:  0.8302\n",
      "\n",
      "Epoch 409 results:\n",
      "Loss  0.5111421401834116\n",
      "Accuracy:  0.8302666666666667\n",
      "\n",
      "Epoch 410 results:\n",
      "Loss  0.5108417771729925\n",
      "Accuracy:  0.8303666666666667\n",
      "\n",
      "Epoch 411 results:\n",
      "Loss  0.5105416392871035\n",
      "Accuracy:  0.8304666666666667\n",
      "\n",
      "Epoch 412 results:\n",
      "Loss  0.5102422931565446\n",
      "Accuracy:  0.8306166666666667\n",
      "\n",
      "Epoch 413 results:\n",
      "Loss  0.5099451032476432\n",
      "Accuracy:  0.8307333333333333\n",
      "\n",
      "Epoch 414 results:\n",
      "Loss  0.5096494057408263\n",
      "Accuracy:  0.8308833333333334\n",
      "\n",
      "Epoch 415 results:\n",
      "Loss  0.5093534111747703\n",
      "Accuracy:  0.8310500000000001\n",
      "\n",
      "Epoch 416 results:\n",
      "Loss  0.5090521022611229\n",
      "Accuracy:  0.8312\n",
      "\n",
      "Epoch 417 results:\n",
      "Loss  0.5087638348761364\n",
      "Accuracy:  0.8312666666666667\n",
      "\n",
      "Epoch 418 results:\n",
      "Loss  0.5084744522911543\n",
      "Accuracy:  0.8313333333333334\n",
      "\n",
      "Epoch 419 results:\n",
      "Loss  0.508194567405682\n",
      "Accuracy:  0.83145\n",
      "\n",
      "Epoch 420 results:\n",
      "Loss  0.5078866837822343\n",
      "Accuracy:  0.8316166666666667\n",
      "\n",
      "Epoch 421 results:\n",
      "Loss  0.5075941215293936\n",
      "Accuracy:  0.8316333333333333\n",
      "\n",
      "Epoch 422 results:\n",
      "Loss  0.507307430497192\n",
      "Accuracy:  0.8318000000000001\n",
      "\n",
      "Epoch 423 results:\n",
      "Loss  0.5070215761357161\n",
      "Accuracy:  0.8318000000000001\n",
      "\n",
      "Epoch 424 results:\n",
      "Loss  0.5067375810666825\n",
      "Accuracy:  0.8318166666666668\n",
      "\n",
      "Epoch 425 results:\n",
      "Loss  0.5064549547543765\n",
      "Accuracy:  0.8318833333333334\n",
      "\n",
      "Epoch 426 results:\n",
      "Loss  0.5061732551864723\n",
      "Accuracy:  0.8320166666666667\n",
      "\n",
      "Epoch 427 results:\n",
      "Loss  0.5058903047233146\n",
      "Accuracy:  0.8320500000000001\n",
      "\n",
      "Epoch 428 results:\n",
      "Loss  0.5056058152423484\n",
      "Accuracy:  0.8321333333333334\n",
      "\n",
      "Epoch 429 results:\n",
      "Loss  0.5053243259549653\n",
      "Accuracy:  0.8321333333333334\n",
      "\n",
      "Epoch 430 results:\n",
      "Loss  0.5050419511340437\n",
      "Accuracy:  0.8322\n",
      "\n",
      "Epoch 431 results:\n",
      "Loss  0.5047637743989791\n",
      "Accuracy:  0.8323333333333334\n",
      "\n",
      "Epoch 432 results:\n",
      "Loss  0.5044879378711391\n",
      "Accuracy:  0.8325\n",
      "\n",
      "Epoch 433 results:\n",
      "Loss  0.504217296074129\n",
      "Accuracy:  0.8325333333333333\n",
      "\n",
      "Epoch 434 results:\n",
      "Loss  0.5039455146419654\n",
      "Accuracy:  0.8325333333333333\n",
      "\n",
      "Epoch 435 results:\n",
      "Loss  0.5036742714260116\n",
      "Accuracy:  0.8325666666666667\n",
      "\n",
      "Epoch 436 results:\n",
      "Loss  0.5033991737827831\n",
      "Accuracy:  0.8326166666666667\n",
      "\n",
      "Epoch 437 results:\n",
      "Loss  0.503158428520014\n",
      "Accuracy:  0.8326166666666667\n",
      "\n",
      "Epoch 438 results:\n",
      "Loss  0.5028866044383143\n",
      "Accuracy:  0.8327\n",
      "\n",
      "Epoch 439 results:\n",
      "Loss  0.5026137229890568\n",
      "Accuracy:  0.83275\n",
      "\n",
      "Epoch 440 results:\n",
      "Loss  0.5023453507332859\n",
      "Accuracy:  0.8329333333333334\n",
      "\n",
      "Epoch 441 results:\n",
      "Loss  0.5020908856643054\n",
      "Accuracy:  0.8329333333333334\n",
      "\n",
      "Epoch 442 results:\n",
      "Loss  0.5018224037458868\n",
      "Accuracy:  0.8329333333333334\n",
      "\n",
      "Epoch 443 results:\n",
      "Loss  0.5015522029770207\n",
      "Accuracy:  0.8330000000000001\n",
      "\n",
      "Epoch 444 results:\n",
      "Loss  0.501279613798656\n",
      "Accuracy:  0.8330000000000001\n",
      "\n",
      "Epoch 445 results:\n",
      "Loss  0.5010081756104596\n",
      "Accuracy:  0.8330000000000001\n",
      "\n",
      "Epoch 446 results:\n",
      "Loss  0.5007404333206229\n",
      "Accuracy:  0.8330666666666667\n",
      "\n",
      "Epoch 447 results:\n",
      "Loss  0.5004709451003724\n",
      "Accuracy:  0.8331833333333334\n",
      "\n",
      "Epoch 448 results:\n",
      "Loss  0.5002055473603617\n",
      "Accuracy:  0.8334166666666667\n",
      "\n",
      "Epoch 449 results:\n",
      "Loss  0.4999365060624598\n",
      "Accuracy:  0.83355\n",
      "\n",
      "Epoch 450 results:\n",
      "Loss  0.4996679444547133\n",
      "Accuracy:  0.8336333333333333\n",
      "\n",
      "Test Set Accuracy\n",
      "Loss  0.5409173213719363\n",
      "Accuracy:  0.8201\n",
      "\n",
      "The training took 73.08121983901947 seconds to finish\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Attempt 10\n",
    "reg_coef = 0.0001\n",
    "lr = 0.01\n",
    "batch_size = 1024\n",
    "epochs = 450\n",
    "layers = [14, 7]\n",
    "loss = \"cross\"\n",
    "last_activation = SOFTMAX\n",
    "\n",
    "train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test on Lin Data\n",
    "lr = 0.0001\n",
    "reg_coef = 0.0001\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "loss= \"l2\"\n",
    "last_activation = None\n",
    "layers=[3]\n",
    "\n",
    "#train_and_test(layers, lr, epochs, batch_size, reg_coef, loss, last_activation, x_train_lin, y_train_lin, x_test_lin, y_test_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
